{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqa84irXnqZi"
   },
   "source": [
    "# Analyzing A Movie Review Dataset(Part 2)[100 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWS7b11Vn9dG"
   },
   "source": [
    "## 0-A. Set-up[-2 if author function is not called]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0EIRM9vnvRP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This assignment is submitted by tpasumarthi3.\n"
     ]
    }
   ],
   "source": [
    "def author(gt_username = 'tpasumarthi3'):\n",
    "  print(\"This assignment is submitted by {0}.\".format(gt_username))\n",
    "#Add your GT_UserName below and uncomment the line.\n",
    "author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJrlQ8Psoagw"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOfan1VXoERu"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awf_c13mR_3A"
   },
   "source": [
    "## 0-B. Text Preprocessing[10(5+5) Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F22uJmPYoFuZ"
   },
   "outputs": [],
   "source": [
    "# Read the train,test and unlabeled data from files and store them into variables train,test and unlabeled_train respectively.\n",
    "### Add your code here. \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "#print (\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    "# \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    "# test[\"review\"].size, unlabeled_train[\"review\"].size ))\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-Ss_Khqpyso"
   },
   "outputs": [],
   "source": [
    "def clean_review(review, remove_stopwords = False):\n",
    "    \"\"\"Helper function to clean the reviews i.e. to convert a document to a sequence of words.\n",
    "     Please note that we're not removing stopwords since word2vec relies on the broader context\n",
    "     of the sentence in order to produce high-quality word vectors.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          remove_stopwards: If true remove stopwords else not. (boolean)\n",
    "     Returns: cleaned_review : Cleaned review (list)\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Remove HTML Tags.\n",
    "     2. Remove non-letter characters.\n",
    "     3. Convert to lower case.\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    cleaned_review= words\n",
    "\n",
    "    #####################\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yb15ZgwB4RNN"
   },
   "outputs": [],
   "source": [
    "def review_to_sentences( review: str, tokenizer: nltk.tokenize.punkt.PunktSentenceTokenizer ):\n",
    "    \"\"\"Helper function to split a review into parsed sentences. Returns a \n",
    "     list of sentences, where each sentence is a list of words.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          tokenizer: punkt tokenizer\n",
    "     Returns:\n",
    "          review_sentences: List of list of tokens.\n",
    "                            e.g. [[\"word2vec\", \"was\", \"introduced\", \"by\", \"google\" ],[\"it\",\"leverages\",\"distributed\",\"token\",\"representations\"]]\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Use the tokenizer to split the paragraph into sentences.\n",
    "     2. Clean the sentence to return a list of words for each sentence using the helper funtion above.\n",
    "     3. Return a list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(clean_review(raw_sentence))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    review_sentences= sentences\n",
    "    ######################\n",
    "    \n",
    "    return review_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi_1vFZ7-sd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Use the above helper functions to convert the reviews in train[\"review\"] and in unlabeled_train[\"review\"] to a list of list format as mentioned above.\n",
    "# For example if your train data contains 2 reviews with 3 sentences in each and the unlabeled_train has 4 reviews with 1 sentence in each\n",
    "# The resultant list should have 10 lists of tokenized sentences.\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "### Add your code here.\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "######################\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "### Add your code here.\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGfJfIH0SVDD"
   },
   "source": [
    "## 1. Word2Vec[80(20*4) Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74UeOdgiURxZ"
   },
   "source": [
    "### a. Create vector representations for each movie post in your training set by training word2vec with context=5, embedding dimension=100, min_words=40. We’ll call the collection of these representations Z1.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmVoyc32IwsI"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_39o3jMeGyKV",
    "outputId": "405775a0-82b2-4aff-fac8-dd3faa29f459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "def generate_z1(sentences,num_features,min_word_count,context,num_workers = 4,downsampling = 1e-3, model_name = \"model1_100features_40minwords_5context\"):\n",
    "    \"\"\"Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "      generate the collection of these representations Z1.\n",
    "      You should carry out the following.\n",
    "      1) Set the parameters as mentioned below. \n",
    "        A)Constrained Paramters : \"context length\", \"embedding dimension\", \"min_words\" (Please check the question for the values.)\n",
    "        B)Optional Parameters: \"number of workers\", \"downsample setting\"\n",
    "      2) Train your word2vec model and save it.\n",
    "      3) Store the collection of word embeddings and the word_list(z1 and word_list_z1) .\n",
    "\n",
    "      Arg: sentences: List of tokenized sentences (List)\n",
    "            num_features: Word vector dimensionality (int)\n",
    "            min_word_count: Minimum word count (int)\n",
    "            context: Context window size (int)\n",
    "            num_workers: Number of threads to run in parallel (int)\n",
    "            downsampling: Downsample setting for frequent words (float)\n",
    "            model_name = Name to save your model (str)\n",
    "      Returns:\n",
    "            trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z1: word embeddings (ndarray)\n",
    "            word_list_z1: List of tokens in the model (List)\n",
    "\n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Training model...\")\n",
    "    ### Add your code here.\n",
    "    \n",
    "    # Initialize and train the model (this will take some time)\n",
    "    trained_word2vec_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    trained_word2vec_model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"100features_40minwords_5context\"\n",
    "    trained_word2vec_model.save(model_name)\n",
    "    \n",
    "    z1 = trained_word2vec_model.wv.syn0\n",
    "    word_list_z1 = trained_word2vec_model.wv.index2word\n",
    "    \n",
    "    #######################\n",
    "\n",
    "    return trained_word2vec_model, z1, word_list_z1\n",
    "\n",
    "# Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "# generate the collection of these representations Z1.\n",
    "# Add the function parameter values below.\n",
    "### Add your code here.\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                 \n",
    "context = 5          # Context window size                   \n",
    "\n",
    "######################\n",
    "\n",
    "model1, z1, word_list_z1 = generate_z1(sentences,num_features,min_word_count,context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hESq0fhVUd7_"
   },
   "source": [
    "### b. Create vector representations for each movie post in your training set by loading the pretrained Google word2vec model. We’ll call the collection of these representations Z2.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zJqisAnJCtA"
   },
   "outputs": [],
   "source": [
    "def generate_z2(word_list_z1):\n",
    "    \"\"\"Use a pre-trained word2vec model(GoogleNews-vectors-negative300) to generate the collection of these representations Z2. Please note Z2 corresponds to the word embeddings for\n",
    "      words that are present in both the previous model and this pre-trained model.\n",
    "\n",
    "      You should carry out the following.\n",
    "\n",
    "      Arg: word_list_z1: List of tokens in the previous trained model (List)\n",
    "      Returns:\n",
    "            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z2: word embeddings (ndarray)\n",
    "            word_list_z2: List of tokens in the model (List)   \n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    pre_trained_word2vec_model= KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    p_list= pre_trained_word2vec_model.index2word\n",
    "    word_list_z2= [word for word in word_list_z1 if word in p_list]\n",
    "    z2= np.zeros((len(word_list_z2), 300))\n",
    "    for i, word in enumerate(word_list_z2):\n",
    "        z2[i,:]= pre_trained_word2vec_model[word]\n",
    "    ######################\n",
    "    return pre_trained_word2vec_model, z2, word_list_z2\n",
    "\n",
    "model2, z2, word_list_z2 = generate_z2(word_list_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9Ug2cErftjY"
   },
   "source": [
    "### c. With k=10, do k-means clustering on each set Z1, Z2. Print a table of the words in each cluster for Z1 and for Z2.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ks_JRyqJrdX"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5UxPcOVgbCF"
   },
   "outputs": [],
   "source": [
    "def fit_kmeans(z,word_list_z,num_clusters = 10):\n",
    "    \"\"\" Fit kmeans on the embedding representations and return a mapping of word to cluster indices. Please use the default values for\n",
    "        the rest of the kmeans parameters.\n",
    "\n",
    "        Arg: z: word embeddings (ndarray)\n",
    "              word_list_z: List of tokens in the model (List) \n",
    "              num_clusters: Number of clusters (int)\n",
    "        Returns:\n",
    "            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z2: word embeddings (ndarray)\n",
    "            word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)      \n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    kmeans_clustering= KMeans(n_clusters= num_clusters)\n",
    "    pred= kmeans_clustering.fit_predict(z)\n",
    "    word_centroid_map_z = dict(zip(word_list_z, pred))\n",
    "    ######################\n",
    "\n",
    "    return word_centroid_map_z\n",
    "\n",
    "\n",
    "word_centroid_map_z1 = fit_kmeans(z1, word_list_z1, 10)\n",
    "word_centroid_map_z2 = fit_kmeans(z2, word_list_z2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZt0ZVXeg9nA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clusters for model1 are....\n",
      "['by', 'american', 'star', 'black', 'classic', 'early', 'directed', 'late', 'writer', 'including', 'novel', 'modern', 'british', 'king', 'french', 'york', 'general', 'e', 'among', 'th', 'de']\n",
      "\n",
      "['death', 'house', 'dead', 'face', 'head', 'white', 'run', 'hand', 'etc', 'car', 'fight', 'blood', 'running', 'room', 'body', 'red', 'killing', 'dog', 'monster', 'hot', 'gun']\n",
      "\n",
      "['john', 'mr', 'michael', 'david', 'robert', 'james', 'o', 'jack', 'george', 'richard', 'tom', 'lee', 'paul', 'peter', 'dr', 'joe', 'bill', 'mark', 'william', 'harry', 'co']\n",
      "\n",
      "['their', 'our', 'war', 'men', 'women', 'against', 'town', 'city', 'group', 'girls', 'police', 'living', 'order', 'middle', 'country', 'local', 'team', 'america', 'space', 'eventually', 'crew']\n",
      "\n",
      "['his', 'who', 'her', 'man', 'young', 'girl', 'woman', 'wife', 'himself', 'father', 'mother', 'boy', 'friend', 'son', 'evil', 'killer', 'child', 'daughter', 'hero', 'husband', 'murder']\n",
      "\n",
      "['the', 'and', 'a', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 'was', 'as', 'for', 'movie', 'but', 'film', 'you', 't', 'on', 'not', 'he']\n",
      "\n",
      "['of', 'both', 'between', 'interesting', 'especially', 'excellent', 'beautiful', 'nice', 'poor', 'boring', 'full', 'stupid', 'small', 'style', 'wonderful', 'often', 'perfect', 'entertaining', 'drama', 'dark', 'despite']\n",
      "\n",
      "['s', 'with', 'has', 'does', 'makes', 'seems', 'gets', 'played', 'found', 'goes', 'comes', 'shows', 'plays', 'takes', 'left', 'used', 'given', 'came', 'playing', 'doing', 'gives']\n",
      "\n",
      "['life', 'world', 'own', 'family', 'human', 'lives', 'self', 'violence', 'lack', 'power', 'relationship', 'reality', 'problems', 'crime', 'tale', 'form', 'sexual', 'deep', 'society', 'romance', 'situation']\n",
      "\n",
      "['have', 'be', 'see', 'get', 'do', 'make', 'think', 'watch', 'know', 'say', 'go', 'find', 'want', 'take', 'give', 'come', 'believe', 'put', 'play', 'seem', 'help']\n",
      "\n",
      "\n",
      "The clusters for model2 are....\n",
      "['by', 'american', 'star', 'black', 'classic', 'early', 'directed', 'late', 'writer', 'including', 'novel', 'modern', 'british', 'king', 'french', 'york', 'general', 'e', 'among', 'th', 'de']\n",
      "\n",
      "['death', 'house', 'dead', 'face', 'head', 'white', 'run', 'hand', 'etc', 'car', 'fight', 'blood', 'running', 'room', 'body', 'red', 'killing', 'dog', 'monster', 'hot', 'gun']\n",
      "\n",
      "['john', 'mr', 'michael', 'david', 'robert', 'james', 'o', 'jack', 'george', 'richard', 'tom', 'lee', 'paul', 'peter', 'dr', 'joe', 'bill', 'mark', 'william', 'harry', 'co']\n",
      "\n",
      "['their', 'our', 'war', 'men', 'women', 'against', 'town', 'city', 'group', 'girls', 'police', 'living', 'order', 'middle', 'country', 'local', 'team', 'america', 'space', 'eventually', 'crew']\n",
      "\n",
      "['his', 'who', 'her', 'man', 'young', 'girl', 'woman', 'wife', 'himself', 'father', 'mother', 'boy', 'friend', 'son', 'evil', 'killer', 'child', 'daughter', 'hero', 'husband', 'murder']\n",
      "\n",
      "['the', 'and', 'a', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 'was', 'as', 'for', 'movie', 'but', 'film', 'you', 't', 'on', 'not', 'he']\n",
      "\n",
      "['of', 'both', 'between', 'interesting', 'especially', 'excellent', 'beautiful', 'nice', 'poor', 'boring', 'full', 'stupid', 'small', 'style', 'wonderful', 'often', 'perfect', 'entertaining', 'drama', 'dark', 'despite']\n",
      "\n",
      "['s', 'with', 'has', 'does', 'makes', 'seems', 'gets', 'played', 'found', 'goes', 'comes', 'shows', 'plays', 'takes', 'left', 'used', 'given', 'came', 'playing', 'doing', 'gives']\n",
      "\n",
      "['life', 'world', 'own', 'family', 'human', 'lives', 'self', 'violence', 'lack', 'power', 'relationship', 'reality', 'problems', 'crime', 'tale', 'form', 'sexual', 'deep', 'society', 'romance', 'situation']\n",
      "\n",
      "['have', 'be', 'see', 'get', 'do', 'make', 'think', 'watch', 'know', 'say', 'go', 'find', 'want', 'take', 'give', 'come', 'believe', 'put', 'play', 'seem', 'help']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_clusters(word_centroid_map_z, model_name):\n",
    "    \"\"\" Print max(20, cluster_size) words for each of the clusters.\n",
    "\n",
    "        Args: word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)  \n",
    "              model_name: Model Name (str)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"The clusters for {0} are....\".format(model_name))\n",
    "    ### Add your code here.\n",
    "    k= len(list(set(word_centroid_map_z.values())))\n",
    "    for c in range(k):\n",
    "        i= 0\n",
    "        words= []\n",
    "        for key, value in word_centroid_map_z1.items():\n",
    "            if(i > 20):\n",
    "                break\n",
    "            if(value == c):\n",
    "                words.append(key)\n",
    "                i=i+1\n",
    "        print(words)\n",
    "        print()\n",
    "    print()\n",
    "    ######################\n",
    "print_clusters(word_centroid_map_z1, \"model1\")\n",
    "print_clusters(word_centroid_map_z2, \"model2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVtybcB0i7er"
   },
   "source": [
    "### d. Featurize the training and test reviews in Z1, Z2 to produce design matrices X1,X2 as described in part 3 of the blog series. Basically, each review is converted into a bag of centroids feature vector with each vector component representing the count of the number of words in that review that belong in that component’s cluster.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cGF5_EZjDUs"
   },
   "outputs": [],
   "source": [
    "# Create tokenized reviews for both train and test. Please note that for this we'll not be splitting the sentences and we'll be removing stopwords.\n",
    "# This is exactly what you did in the preprocessing step for HW1\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []\n",
    "### Add your code here.\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(clean_review(review, remove_stopwords=True))\n",
    "    \n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(clean_review(review, remove_stopwords=True))\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGuPb-Hplp5A"
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(tokenized_review, word_centroid_map, num_clusters = 10):\n",
    "    \"\"\" Create a bag of kmeans centroids for each review i.e. for a review we return an array of length num_clusters with each \n",
    "        element of the array indicating how many words(tokens) of the review belong to that cluster.\n",
    "\n",
    "        Args: tokenized_review: list of tokens corresponding to a review (List)\n",
    "              word_centroid_map: word to cluster_index map for the model (Dict)\n",
    "              num_clusters: Number of clusters (int)\n",
    "        \n",
    "        Returns: bag_of_centroids: An array containing the count of tokens in each cluster (ndarray)\n",
    "    \"\"\"\n",
    "  \n",
    "    ### Add your code here.\n",
    "    bag_of_centroids= np.zeros(num_clusters)\n",
    "    \n",
    "    for word in tokenized_review:\n",
    "        if word in word_centroid_map:\n",
    "            idx= word_centroid_map[word]\n",
    "            bag_of_centroids[idx]= bag_of_centroids[idx]+1\n",
    "    #######################\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EImSAI6LvkeS"
   },
   "outputs": [],
   "source": [
    "def create_design_matrices(data, cleaned_reviews, word_centroid_map_z1, word_centroid_map_z2, num_clusters = 10):\n",
    "    \"\"\" Creates the design matrices X1(for trained model) and X2(for pretrained google word2vec model) for the given data\n",
    "\n",
    "        Args: data: Train/Test data (pandas.core.frame.DataFrame)\n",
    "              cleaned_reviews: List of tokenized reviews(sentences are not split and stopwords removed) (List)\n",
    "              word_centroid_map_z1: word to cluster map for model1 (Dict)\n",
    "              word_centroid_map_z2: word to cluster map for model2 (Dict)\n",
    "              num_clusters: Number of KMeans clusters (int)\n",
    "\n",
    "        Returns:\n",
    "              x1_data: Design matrices X1-- Shape should be num_reviews*num_clusters (np.ndarray) \n",
    "              x2_data: Design matrices X2-- Shape should be num_reviews*num_clusters (np.ndarray)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    x1_data= np.zeros((data[\"review\"].size, num_clusters))\n",
    "    x2_data= np.zeros((data[\"review\"].size, num_clusters))\n",
    "    \n",
    "    for i, review in enumerate(cleaned_reviews):\n",
    "        x1_data[i]= create_bag_of_centroids(review, word_centroid_map_z1)\n",
    "        x2_data[i]= create_bag_of_centroids(review, word_centroid_map_z2)\n",
    "    ######################\n",
    "    return x1_data,x2_data\n",
    "x1_train,x2_train = create_design_matrices(train,clean_train_reviews,word_centroid_map_z1,word_centroid_map_z2,10)\n",
    "x1_test,x2_test = create_design_matrices(test,clean_test_reviews,word_centroid_map_z1,word_centroid_map_z2,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPzXOj2ryAQW"
   },
   "source": [
    "### e. Save X1, X2 for both train and test set.[-10 if missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOFv1becyFDs"
   },
   "outputs": [],
   "source": [
    "#Save the design matrices for both train and test set as .npy files and submit these as part of the delieverables\n",
    "### Add your code here.\n",
    "np.save('x1_train.npy', x1_train)\n",
    "np.save('x1_test.npy', x1_test)\n",
    "np.save('x2_train.npy', x2_train)\n",
    "np.save('x2_test.npy', x2_test)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCTKysWnwA_C"
   },
   "source": [
    "## 2. Classification Experiment[10(5+3+2) Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BhBJKsdB_Jq"
   },
   "source": [
    "### a. Properly train and tune a collection of random forest classifiers using cross-validation for the design matrices X1 and X2. You should end up with two classifiers, M1 and M2. Print the best f1_scores on the test set for both the design matrices.[8 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USTbwme0HkZF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SXt7Bh5pv62g",
    "outputId": "21b0c689-1640-4094-bb69-9b51d1ea34c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Get the value for y from the original dataframe\n",
    "y_train = train['sentiment'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWbr-MZgwI_4"
   },
   "outputs": [],
   "source": [
    "def split(X, label):\n",
    "    \"\"\"Helper function to create a train-test split of the data.(test_size = 0.2, random_state = 0)\n",
    "\n",
    "     Arg: X: Design Matrix(ndarray)\n",
    "          label: sentiment values\n",
    "     Returns:\n",
    "          x_train: train input features\n",
    "          x_test: test input features\n",
    "          y_train: train labels\n",
    "          y_test: test labels\n",
    "    \"\"\"\n",
    "    ###Add your code here.\n",
    "    x_train, x_test, y_train, y_test= train_test_split(X, label, test_size=0.2, random_state=0)\n",
    "    #####################\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPD5H-qcwRxD"
   },
   "outputs": [],
   "source": [
    "# Call split function for both the design matrices x1 and x2 obtained from Q1. \n",
    "###Add your code here.\n",
    "x1_train_split, x1_test_split, y1_train_split, y1_test_split= split(x1_train, y_train)\n",
    "x2_train_split, x2_test_split, y2_train_split, y2_test_split= split(x2_train, y_train)\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBnc29crwU7M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\tyco9\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def cross_val(x_train_split,y_train_split,n_estimators):\n",
    "    \"\"\"function to calculate the cross validation scores for the 2 values of x_train, and y_train on the Random \n",
    "        Forest Classifier.\n",
    "\n",
    "     Arg: x_train_split: train input features\n",
    "          y_train_split: train labels\n",
    "          n_estimators: List of estimators to perform Random Forest with.\n",
    "     Returns:\n",
    "          cross_val_scores: List of the CV scores the estimators (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Add your code here.\n",
    "    cross_val_scores= []\n",
    "    for n in n_estimators:\n",
    "        clf= RandomForestClassifier(n_estimators= n)\n",
    "        scores= cross_val_score(clf, x_train_split, y_train_split)\n",
    "        cross_val_scores.append(scores.mean())\n",
    "    #####################\n",
    "    return cross_val_scores\n",
    "n_estimators = [50, 100, 150, 200, 300, 400, 500, 750, 1000]\n",
    "result1 = cross_val(x1_train_split,y1_train_split,n_estimators) #CV scores for X1\n",
    "result2 = cross_val(x2_train_split,y2_train_split,n_estimators) #CV scores for X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yUt2c_uswkJX",
    "outputId": "aa416b5d-b38b-4ae1-e897-55572431d7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score using X1 as input is 0.6400961345884238\n",
      "The best f1 score using X2 as input is 0.7718298392021169\n"
     ]
    }
   ],
   "source": [
    "def cal_f1(x_train_split,y_train_split,x_test_split,y_test_split,result,n_estimators):\n",
    "    \"\"\"function to calculate the best F-1 score based on the best value of n_estimators obtained from cross-val.\n",
    "\n",
    "     Arg: x_train_split: train input features\n",
    "          y_train_split: train labels\n",
    "          x_test_split: test input features\n",
    "          y_test_split: test labels\n",
    "          n_estimators: List of estimators to perform Random Forest with.\n",
    "     Returns:\n",
    "          f1: f1_score for the best value of n_estimator obtained from cross-val (float)\n",
    "    \"\"\"\n",
    "    ###Add your code here.\n",
    "    n= n_estimators[result.index(max(result))]\n",
    "    clf= RandomForestClassifier(n_estimators= n)\n",
    "    clf.fit(x_train_split, y_train_split)\n",
    "    y_pred= clf.predict(x_test_split)\n",
    "    f1= f1_score(y_test_split, y_pred)\n",
    "    #####################\n",
    "    return f1\n",
    "f1_first = cal_f1(x1_train_split,y1_train_split,x1_test_split,y1_test_split,result1,n_estimators)\n",
    "f1_second = cal_f1(x2_train_split,y2_train_split,x2_test_split,y2_test_split,result2,n_estimators)\n",
    "print(\"The best f1 score using X1 as input is {0}\".format(f1_first))\n",
    "print(\"The best f1 score using X2 as input is {0}\".format(f1_second))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isKMNQyTCZIT"
   },
   "source": [
    "### b. Which featurization technique works best for sentiment classification? Is this better or worse than the simple bag-of-words approach? What are at least three things you could do to improve the efficacy of the classifier?[2 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGlmBKgFCaSp"
   },
   "source": [
    "The pretrained google word2vec model works better than our trained model, as it has a higher F1 score. This is much better than the bag-of-words model. Three thing that we could do to improve the efficacy is increasing the amount of data we used to train the model, incorporating ensebles learning methods with boosting, and increasing the embedding dimensions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2_CSE_6240_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
